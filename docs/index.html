<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Musi</title>
    <meta charset="utf-8" />
    <meta name="author" content="Terry Wang, Rima Mittal, Joshua Goldberg  Time Series Analysis &amp; Forecasting Spring 2019" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/robot-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="uchicago.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">






class: title-slide

&lt;br&gt;&lt;br&gt;

&lt;img src="https://grahamschool.uchicago.edu/themes/custom/ts_uchi/images/svgs/logo.svg" width="75%"/&gt;

# .title-slide-h1[Human + AI Music Generation]
### .title-slide-h2[An analytical approach to original music creation]
.title-slide-h3[Terry Wang, Rima Mittal, Joshua Goldberg]

.title-slide-h3[Research Design &amp; Business Applications | Spring 2019]

---

class: text-slide, center, middle

# Business opportunity

--

.pull-left[

&lt;img src="https://i.ytimg.com/vi/XJzTF1CWwA0/maxresdefault.jpg" width="400px" height="250px"/&gt;

]

--

.pull-right[

&lt;img src="https://media.giphy.com/media/LrKReGHYh6rTy/giphy.gif" width="400px" height="250px"/&gt;

]

---

class: text-slide

# Research Purpose

The purpose of this project is to develop a deep learning based music generator (instrumental music) which will help musicians and non-musicians alike to develop and refine musical ideas, and facilitate them in the music composition process. 

--

## Deliverable outline

--


* A GAN/Variational AutoEncoder music generation model either using a time series approach or another analytical method.


--


* A framework (genetic algorithm) to update weights/parameters of the model. We plan to have the human user choose k "good" samples from n generated samples for each iteration.


--


* A user interface where the user can interact with the model.

---

class: text-slide, center, middle

# Two key parts

.pull-left[

&lt;img src="https://r.hswstatic.com/w_1024/gif/electricity-quiz-orig.jpg" width="400px" height="250px"/&gt;

]

--

.pull-right[

&lt;img src="http://clipart-library.com/data_images/182197.jpg" width="400px" height="250px"/&gt;

]

---

class: text-slide

# Data and model description

--


* 130,000 midi files curated by a Reddit user, with a size of 3.65GB uncompressed. The collection has a great amount of variety representing all major genres in classical, jazz, metal, pop, etc.


--


* The generative algorithm will be of recurrent nature, where a snippet of monophonic midi track consisting of n number of quarter or `\(\frac{1}{16^\text{th}}\)` notes will be fed at a time into a neural network of input matrix dimension `\(129 \times n\)`.


* This neural network will then output a matrix `\(129 \times 1\)`, which will be compared to the immediate next quarter or `\(\frac{1}{16^\text{th}}\)` note of the input for back propagation. Thus, the model would take n quarter or `\(\frac{1}{16^\text{th}}\)` notes as input to predict the immediate next note.


--


* The adversarial algorithm is a simple discriminator to find whether a given input is from one of the original training samples or not.


???

First bullet point: The music data we have is a collection of 130,000 music files. The Genres include: Pop, Classical (Piano/Violin/Guitar), EDM, VideoGame, Movie/TV Theme.

Second bullet: The length is yet to be determined and will be determined through experimentation

Last bullet: The length of the generated music is yet to be determined, but in theory we should be able to generate music of unlimited length given the recurrent structure.

---

class: text-slide

# Visualizing model structure

.center[&lt;img src="img/rnn_uchicago_colors.gif" height="450px"&gt;]

.footnote[Source: https://brangerbriz.com/blog/using-machine-learning-to-create-new-melodies]

---

class: text-slide

# Definition of variables

Python libraries like __Music21__, __Librosa__, __PrettyMIDI__ are the toolkits we are using to extract meaningful information from our MIDI files. 

&lt;br&gt;

The stream object extracted using __Music21__ can then be used to identify:

* Instruments

* Key Signatures

* Overlaps

* Time Signatures

* Measures

From the measure, we were then able to identify:

* Notes

* Chord

???

We are also working with other metrics like the Pitch Histogram and other Composition Parameters Analysis.

---

class: text-slide

# To-date progress

--


* Gathered data and performed initial analysis


--


* Fit rudimentary model to a small sample


--


* Analyzed song structure with time series approaches


--

---

class: text-slide

# Technical challenges

--


* Novelty of the project results in less formulated resources to rely on


--


* Determining a flexible model architecture to allow user feedback


--


* Collecting more data for training
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
